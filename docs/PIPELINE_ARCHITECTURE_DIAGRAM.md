# SNOWFLAKE VIDEO PIPELINE: FULL ARCHITECTURE & GAP ANALYSIS

**Generated:** 2026-02-16
**Status:** Shot Engine complete, Video Engine missing, Critical bugs in shot list generation

---

## 1. FULL PIPELINE ARCHITECTURE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SNOWFLAKE VIDEO GENERATION PIPELINE                      â”‚
â”‚                    (Story â†’ Screenplay â†’ Shots â†’ Video Clips)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: SCREENPLAY ENGINE (Save the Cat Method)                                â”‚
â”‚ Location: src/screenplay_engine/                                                 â”‚
â”‚ Status: âœ… COMPLETE (v2.0.0, 970 tests passing)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”‚  INPUT: User seed idea
  â”‚
  â”œâ”€â–º Step 1: Logline (Ch.1)
  â”‚     â””â”€â–º sp_step_1_logline.json
  â”‚
  â”œâ”€â–º Step 2: Genre Classification (Ch.2)
  â”‚     â””â”€â–º sp_step_2_genre.json
  â”‚
  â”œâ”€â–º Step 3: Hero Construction (Ch.3)
  â”‚     â””â”€â–º sp_step_3_hero.json (hero, antagonist, b_story_character)
  â”‚
  â”œâ”€â–º Step 3b: World Bible (NEW)
  â”‚     â””â”€â–º sp_step_3b_world_bible.json
  â”‚         â”œâ”€ arena (rules, time_period, scope)
  â”‚         â”œâ”€ geography (landscape, climate, key_locations[])
  â”‚         â”œâ”€ social_structure (class_system, power_dynamics, tensions)
  â”‚         â”œâ”€ economy (how_people_earn, scarcity_abundance)
  â”‚         â”œâ”€ culture (values, customs, taboos)
  â”‚         â”œâ”€ daily_life (morning_rhythm, sensory_palette)
  â”‚         â””â”€ rules_of_conflict (story_engine, systemic_pressure, stakes)
  â”‚
  â”œâ”€â–º Step 3c: Full Cast (NEW, LAYERED)
  â”‚     â””â”€â–º sp_step_3c_full_cast.json
  â”‚         â”œâ”€ tier_1_major_supporting[] (5+ scenes, arcs, detailed bios)
  â”‚         â”œâ”€ tier_2_minor_supporting[] (2-4 scenes, specific functions)
  â”‚         â””â”€ tier_3_background_types[] (archetypes, population flavor)
  â”‚
  â”œâ”€â–º Step 4: Beat Sheet (Ch.4)
  â”‚     â””â”€â–º sp_step_4_beat_sheet.json (15 STC beats with %targets)
  â”‚
  â”œâ”€â–º Step 5: Board (Ch.5)
  â”‚     â””â”€â–º sp_step_5_board.json (scene cards across 4 rows/acts)
  â”‚
  â”œâ”€â–º Step 5b: Visual Bible (NEW)
  â”‚     â””â”€â–º sp_step_5b_visual_bible.json â—„â”€â”€â”€â”€â”€â”
  â”‚         â”œâ”€ style_bible                      â”‚ CRITICAL FOR
  â”‚         â”‚   â”œâ”€ visual_tone                  â”‚ IMAGE/VIDEO
  â”‚         â”‚   â”œâ”€ color_palette                â”‚ GENERATION
  â”‚         â”‚   â”œâ”€ lighting_style               â”‚
  â”‚         â”‚   â”œâ”€ texture_grain                â”‚
  â”‚         â”‚   â”œâ”€ reference_films[]            â”‚
  â”‚         â”‚   â”œâ”€ shape_language               â”‚
  â”‚         â”‚   â””â”€ do_not[] (negative prompts)  â”‚
  â”‚         â”œâ”€ color_script[] (per-act mood)    â”‚
  â”‚         â”œâ”€ location_designs[]               â”‚
  â”‚         â”‚   â”œâ”€ location_name                â”‚
  â”‚         â”‚   â”œâ”€ visual_description           â”‚
  â”‚         â”‚   â”œâ”€ time_variants{}              â”‚
  â”‚         â”‚   â”œâ”€ color_sub_palette[]          â”‚
  â”‚         â”‚   â”œâ”€ mood_keywords[]              â”‚
  â”‚         â”‚   â””â”€ t2i_base_prompt â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”€â”€â”
  â”‚         â”œâ”€ character_visual_notes[]         â”‚  â”‚
  â”‚         â”‚   â”œâ”€ character_name               â”‚  â”‚
  â”‚         â”‚   â”œâ”€ physical_summary             â”‚  â”‚
  â”‚         â”‚   â”œâ”€ wardrobe_evolution           â”‚  â”‚
  â”‚         â”‚   â”œâ”€ signature_visual_identifier  â”‚  â”‚
  â”‚         â”‚   â””â”€ t2i_portrait_prompt â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”€â”€â”¤â”€ T2I Prompts
  â”‚         â””â”€ cinematography_approach          â”‚  â”‚  (for Flux/SD)
  â”‚             â”œâ”€ default_lens                 â”‚  â”‚
  â”‚             â”œâ”€ handheld_vs_dolly            â”‚  â”‚
  â”‚             â”œâ”€ depth_of_field               â”‚  â”‚
  â”‚             â””â”€ aspect_ratio                 â”‚  â”‚
  â”‚                                              â”‚  â”‚
  â”œâ”€â–º Step 6: Screenplay (end Ch.5)             â”‚  â”‚
  â”‚     â””â”€â–º sp_step_6_screenplay.json           â”‚  â”‚
  â”‚                                              â”‚  â”‚
  â”œâ”€â–º Step 7: 7 Immutable Laws (Ch.6)           â”‚  â”‚
  â”‚     â””â”€â–º sp_step_7_laws.json                 â”‚  â”‚
  â”‚                                              â”‚  â”‚
  â”œâ”€â–º Step 8: 9 Diagnostic Checks (Ch.7)        â”‚  â”‚
  â”‚     â””â”€â–º sp_step_8_diagnostics.json          â”‚  â”‚
  â”‚                                              â”‚  â”‚
  â””â”€â–º Step 9: Marketing (Ch.8)                  â”‚  â”‚
      â””â”€â–º sp_step_9_marketing.json              â”‚  â”‚
                                                 â”‚  â”‚
                                                 â”‚  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: SHOT ENGINE (Screenplay â†’ Shot List) â”‚  â”‚                            â”‚
â”‚ Location: src/shot_engine/                    â”‚  â”‚                            â”‚
â”‚ Status: âœ… COMPLETE (v12.0.0)                 â”‚  â”‚                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚                                              â”‚  â”‚
  â”‚  INPUT: sp_step_8_screenplay.json           â”‚  â”‚
  â”‚         sp_step_3_hero.json                 â”‚  â”‚
  â”‚         sp_step_5b_visual_bible.json â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â”‚         sp_step_3b_world_bible.json            â”‚
  â”‚         sp_step_3c_full_cast.json              â”‚
  â”‚                                                 â”‚
  â”œâ”€â–º V1: Scene Decomposition (step_v1_decomposition.py)
  â”‚     - Parse screenplay scenes into shot segments
  â”‚     - Detect content triggers (dialogue, action, reveals, etc.)
  â”‚     - Extract characters_in_frame from action text
  â”‚     - Classify emotional_intensity, is_disaster_moment
  â”‚     â””â”€â–º Intermediate: List[ShotSegment]
  â”‚
  â”œâ”€â–º V2: Shot Type Assignment (step_v2_shot_types.py)
  â”‚     - Map trigger â†’ default shot type (wide, close-up, etc.)
  â”‚     - Apply action patterns (shot variety cycle)
  â”‚     - Adjust for emotional intensity
  â”‚     â””â”€â–º Updated: shot.shot_type
  â”‚
  â”œâ”€â–º V3: Camera Behavior (step_v3_camera.py)
  â”‚     - Map trigger â†’ camera movement (static, tracking, push-in)
  â”‚     - Apply cinematography rules from visual_bible
  â”‚     - Set lens_mm, camera_height, distance_band
  â”‚     â””â”€â–º Updated: shot.camera_movement, lens_mm, etc.
  â”‚
  â”œâ”€â–º V4: Duration & Pacing (step_v4_pacing.py)
  â”‚     - Calculate raw durations from trigger + dialogue length
  â”‚     - Apply beat-based pacing curve (moderate/rapid/etc.)
  â”‚     - Scale to match scene target_duration_seconds
  â”‚     â””â”€â–º Updated: shot.duration_seconds, pacing_curve
  â”‚
  â”œâ”€â–º V5: Transition Planning (step_v5_transitions.py)
  â”‚     - Determine cut vs dissolve vs fade
  â”‚     - Apply beat-level transition rules
  â”‚     - Set crossfade durations
  â”‚     â””â”€â–º Updated: shot.transition_to_next, crossfade_duration
  â”‚
  â””â”€â–º V6: Prompt Generation (step_v6_prompts.py) â—„â”€â”€â”€â”€â”
        - Normalize slugline â†’ setting_ref_id          â”‚
        - Normalize characters_in_frame â†’ character_ref_ids
        - Build scene_prompt (I2I edit instruction)    â”‚
        - Build video_prompt (I2V motion description)  â”‚
        - Pull t2i_base_prompt from visual_bible â”€â”€â”€â”€â”€â”€â”˜
        - Pull t2i_portrait_prompt from visual_bible
        - Build negative_prompt from style_bible.do_not[]
        â””â”€â–º OUTPUT: shot_list.json (754 shots for 40-scene screenplay)
              â”‚
              â”œâ”€ project_id, title, format, total_shots, total_duration_seconds
              â””â”€ scenes[]
                  â”œâ”€ scene_number, slugline, beat, emotional_polarity
                  â”œâ”€ visual_intent{emotional_start, emotional_end, conflict_axis, etc.}
                  â”œâ”€ target_duration_seconds
                  â””â”€ shots[]  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”œâ”€ shot_id (s001_001, s001_002, ...)                    â”‚
                      â”œâ”€ trigger, content, dialogue_text, dialogue_speaker    â”‚
                      â”œâ”€ characters_in_frame[] â—„â”€â”€ âš ï¸ BUG: often empty       â”‚
                      â”œâ”€ shot_type, camera_movement, lens_mm                  â”‚
                      â”œâ”€ duration_seconds, pacing_curve                       â”‚
                      â”œâ”€ transition_to_next, crossfade_duration               â”‚
                      â”‚                                                        â”‚
                      â”‚  â”€â”€ V6 PROMPT OUTPUTS (for visual bible) â”€â”€           â”‚
                      â”œâ”€ setting_ref_id (e.g., "east_la_strip_mall_...")     â”‚
                      â”œâ”€ character_ref_ids[] (e.g., ["rae_calder", ...])     â”‚
                      â”œâ”€ character_prompt_prefix (T2I portrait prompt)        â”‚
                      â”œâ”€ scene_prompt (I2I: character placement/blocking) â—„â”€â”€â”€â”¤â”€â”€ TO VIDEO ENGINE
                      â”œâ”€ video_prompt (I2V: motion verbs) â—„â”€â”€ âš ï¸ BUG         â”‚
                      â”œâ”€ negative_prompt (style_bible.do_not[])              â”‚
                      â”œâ”€ ambient_description (audio cues)                     â”‚
                      â”œâ”€ init_image_source (reference|previous_frame|gen)     â”‚
                      â””â”€ aspect_ratio ("16:9")                                â”‚
                                                                              â”‚
                                                                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”
â”‚ PHASE 3: VISUAL BIBLE ASSET MANAGER (Image Cache & State Tracking)         â”‚   â”‚
â”‚ Location: âŒ MISSING â€” needs to be built                                   â”‚   â”‚
â”‚ Status: ğŸš§ NOT IMPLEMENTED                                                 â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”˜
  â”‚                                                                           â”‚
  â”‚  INPUT: shot_list.json + sp_step_5b_visual_bible.json                    â”‚
  â”‚                                                                           â”‚
  â”œâ”€â–º Asset Index Builder                                                    â”‚
  â”‚     - Parse visual_bible.location_designs[]                              â”‚
  â”‚     - Parse visual_bible.character_visual_notes[]                        â”‚
  â”‚     - Create normalized lookup tables:                                   â”‚
  â”‚       â””â”€ settings_index: {setting_ref_id â†’ t2i_base_prompt}              â”‚
  â”‚       â””â”€ characters_index: {character_ref_id â†’ t2i_portrait_prompt}      â”‚
  â”‚                                                                           â”‚
  â”œâ”€â–º Setting Image Generator (T2I â€” ONCE per location)                      â”‚
  â”‚     - For each unique setting_ref_id in shot_list:                       â”‚
  â”‚       1. Check cache: artifacts/{project_id}/settings/{ref_id}.png       â”‚
  â”‚       2. If missing: call Flux/SD with t2i_base_prompt                   â”‚
  â”‚       3. Save to cache                                                   â”‚
  â”‚     â””â”€â–º OUTPUT: Cached setting images (e.g., 12 unique locations)        â”‚
  â”‚                                                                           â”‚
  â”œâ”€â–º Character Portrait Generator (T2I â€” ONCE per character)                â”‚
  â”‚     - For each unique character_ref_id in shot_list:                     â”‚
  â”‚       1. Check cache: artifacts/{project_id}/characters/{ref_id}.png     â”‚
  â”‚       2. If missing: call Flux/SD with t2i_portrait_prompt               â”‚
  â”‚       3. Save to cache                                                   â”‚
  â”‚     â””â”€â–º OUTPUT: Cached character portraits (e.g., 15 named characters)   â”‚
  â”‚                                                                           â”‚
  â””â”€â–º State Manifest Writer                                                  â”‚
        - Write visual_bible_manifest.json:                                  â”‚
          {                                                                   â”‚
            "settings": {                                                     â”‚
              "east_la_strip_mall_parking_lot": {                            â”‚
                "image_path": "settings/east_la_strip_mall_parking_lot.png", â”‚
                "prompt": "...",                                              â”‚
                "generated_at": "..."                                         â”‚
              }                                                               â”‚
            },                                                                â”‚
            "characters": {                                                   â”‚
              "rae_calder": {                                                 â”‚
                "image_path": "characters/rae_calder.png",                    â”‚
                "prompt": "...",                                              â”‚
                "generated_at": "..."                                         â”‚
              }                                                               â”‚
            }                                                                 â”‚
          }                                                                   â”‚
        â””â”€â–º OUTPUT: visual_bible_manifest.json                               â”‚
                                                                              â”‚
                                                                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”
â”‚ PHASE 4: VIDEO ENGINE (Shot â†’ Video Clip)                                  â”‚   â”‚
â”‚ Location: âŒ MISSING â€” needs to be built at i2v/ or src/video_engine/      â”‚   â”‚
â”‚ Status: ğŸš§ NOT IMPLEMENTED                                                 â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”˜
  â”‚                                                                           â”‚
  â”‚  INPUT: shot_list.json + visual_bible_manifest.json                      â”‚
  â”‚                                                                           â”‚
  â”œâ”€â–º Shot Processor (PER SHOT, 754 iterations for full feature)             â”‚
  â”‚     For each shot in shot_list.scenes[].shots[]:                         â”‚
  â”‚                                                                           â”‚
  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
  â”‚     â”‚ STEP A: LOAD SETTING IMAGE                              â”‚         â”‚
  â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚
  â”‚     â”‚  setting_img = load_cached(shot.setting_ref_id)         â”‚         â”‚
  â”‚     â”‚  # E.g., "settings/east_la_strip_mall_parking_lot.png"  â”‚         â”‚
  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
  â”‚                                                                           â”‚
  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
  â”‚     â”‚ STEP B: INPAINT CHARACTERS INTO SETTING (I2I)           â”‚         â”‚
  â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚
  â”‚     â”‚  IF shot.characters_in_frame is not empty:              â”‚         â”‚
  â”‚     â”‚    1. Load character portraits from cache:              â”‚         â”‚
  â”‚     â”‚       char_imgs = [load_cached(ref_id)                  â”‚         â”‚
  â”‚     â”‚                    for ref_id in shot.character_ref_ids] â”‚         â”‚
  â”‚     â”‚                                                           â”‚         â”‚
  â”‚     â”‚    2. Call inpainting model (SD Inpaint / Flux Inpaint):â”‚         â”‚
  â”‚     â”‚       composed_img = inpaint(                            â”‚         â”‚
  â”‚     â”‚         base_image=setting_img,                          â”‚         â”‚
  â”‚     â”‚         character_images=char_imgs,                      â”‚         â”‚
  â”‚     â”‚         edit_prompt=shot.scene_prompt,  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”     CRITICAL:
  â”‚     â”‚         # E.g., "RAE CALDER (lean, wiry-strong, dark    â”‚    â”‚     scene_prompt
  â”‚     â”‚         #       hair, scar) center frame, facing camera"â”‚    â”‚     tells WHERE to
  â”‚     â”‚         negative_prompt=shot.negative_prompt             â”‚    â”‚     place characters
  â”‚     â”‚       )                                                  â”‚    â”‚     in the setting
  â”‚     â”‚  ELSE:                                                   â”‚    â”‚
  â”‚     â”‚    composed_img = setting_img  # no characters          â”‚    â”‚
  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
  â”‚                                                                     â”‚
  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚     â”‚ STEP C: GENERATE VIDEO CLIP (I2V)                         â”‚ â”‚
  â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
  â”‚     â”‚  video_clip = call_veo_api(                               â”‚ â”‚
  â”‚     â”‚    init_image=composed_img,  â—„â”€ from Step B               â”‚ â”‚
  â”‚     â”‚    motion_prompt=shot.video_prompt,  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”
  â”‚     â”‚    # E.g., "camera panning right, slow environmental     â”‚   â”‚  CRITICAL:
  â”‚     â”‚    #       reveal, ambient motion, 8.0s"                 â”‚   â”‚  video_prompt
  â”‚     â”‚    duration=shot.duration_seconds,                       â”‚   â”‚  describes MOTION
  â”‚     â”‚    aspect_ratio=shot.aspect_ratio,                       â”‚   â”‚  not composition
  â”‚     â”‚    negative_prompt=shot.negative_prompt,                 â”‚   â”‚
  â”‚     â”‚    generation_profile=shot.generation_profile            â”‚   â”‚
  â”‚     â”‚  )                                                        â”‚   â”‚
  â”‚     â”‚                                                           â”‚   â”‚
  â”‚     â”‚  # Save to disk:                                         â”‚   â”‚
  â”‚     â”‚  output_path = f"clips/{shot.shot_id}.mp4"               â”‚   â”‚
  â”‚     â”‚  video_clip.save(output_path)                            â”‚   â”‚
  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
  â”‚                                                                     â”‚
  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚     â”‚ STEP D: AUDIO PIPELINE (PARALLEL)                         â”‚ â”‚
  â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
  â”‚     â”‚  IF shot.dialogue_text:                                   â”‚ â”‚
  â”‚     â”‚    1. Generate TTS:                                       â”‚ â”‚
  â”‚     â”‚       tts_audio = elevenlabs.generate(                    â”‚ â”‚
  â”‚     â”‚         text=shot.dialogue_text,                          â”‚ â”‚
  â”‚     â”‚         voice_id=map_speaker_to_voice(                    â”‚ â”‚
  â”‚     â”‚           shot.dialogue_speaker                           â”‚ â”‚
  â”‚     â”‚         )                                                 â”‚ â”‚
  â”‚     â”‚       )                                                   â”‚ â”‚
  â”‚     â”‚    2. Align lips (optional):                             â”‚ â”‚
  â”‚     â”‚       video_clip = wav2lip(video_clip, tts_audio)        â”‚ â”‚
  â”‚     â”‚                                                           â”‚ â”‚
  â”‚     â”‚  # Generate ambient audio from shot.ambient_description  â”‚ â”‚
  â”‚     â”‚  ambient_audio = generate_ambient(                        â”‚ â”‚
  â”‚     â”‚    shot.ambient_description                              â”‚ â”‚
  â”‚     â”‚    # E.g., "outdoor ambiance, crickets, distant traffic" â”‚ â”‚
  â”‚     â”‚  )                                                        â”‚ â”‚
  â”‚     â”‚                                                           â”‚ â”‚
  â”‚     â”‚  # Mix audio layers:                                     â”‚ â”‚
  â”‚     â”‚  final_audio = mix(                                       â”‚ â”‚
  â”‚     â”‚    dialogue=tts_audio if exists,                         â”‚ â”‚
  â”‚     â”‚    ambient=ambient_audio,                                â”‚ â”‚
  â”‚     â”‚    music=None  # TODO: score engine                      â”‚ â”‚
  â”‚     â”‚  )                                                        â”‚ â”‚
  â”‚     â”‚                                                           â”‚ â”‚
  â”‚     â”‚  # Attach to video:                                      â”‚ â”‚
  â”‚     â”‚  video_clip.set_audio(final_audio)                       â”‚ â”‚
  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
  â”‚                                                                     â”‚
  â”‚     â””â”€â–º OUTPUT: clips/{shot_id}.mp4 (with audio)                   â”‚
  â”‚                                                                     â”‚
  â”œâ”€â–º Shot Stitcher (Assemble full scenes/sequences)                   â”‚
  â”‚     - For each scene in shot_list:                                 â”‚
  â”‚       1. Load all shot clips: clips/s{scene_num}_*.mp4             â”‚
  â”‚       2. Apply transitions (cut / dissolve / fade)                 â”‚
  â”‚       3. Concatenate with crossfade_duration if specified          â”‚
  â”‚       4. Save: scenes/scene_{scene_num}.mp4                        â”‚
  â”‚     â””â”€â–º OUTPUT: scenes/ directory with 40 scene files              â”‚
  â”‚                                                                     â”‚
  â””â”€â–º Final Assembly                                                    â”‚
        - Concatenate all scenes/scene_*.mp4                            â”‚
        - Apply act-level transitions (fade to black between acts)     â”‚
        - Add opening/closing titles (from sp_step_9_marketing.json)   â”‚
        - Export final: output/{project_id}_FINAL.mp4                  â”‚
        â””â”€â–º OUTPUT: Full feature film (~110 minutes, ~6929s)            â”‚
                                                                        â”‚
                                                                        â”‚
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”˜

    FINAL OUTPUT: artifacts/{project_id}_FINAL.mp4
    Duration: ~115 minutes for feature film
    Shots: 754 video clips stitched with transitions
    Audio: Dialogue (TTS) + Ambient + Music (future)
    Quality: Production-ready 16:9 video
```

---

## 2. PER-SHOT EXECUTION FLOW (Runtime Detail)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RUNTIME: Processing Shot s001_003 (Scene 1, Shot 3)                   â”‚
â”‚ "Rae exits the taqueria, slides behind a dumpster, pulls baton"       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT DATA (from shot_list.json):
  shot_id: "s001_003"
  setting_ref_id: "east_la_strip_mall_parking_lot"
  character_ref_ids: ["rae_calder"]
  scene_prompt: "RAE CALDER (lean, wiry-strong, dark hair, scar) center frame,
                 figure exits the taqueria, rae slides behind a dumpster, she pulls
                 a telescoping baton, medium shot, subject at medium distance,
                 predatory, working-class, Sodium amber, warm tones"
  video_prompt: "camera slowly pushing in, figure exits the taqueria, rae slides
                 behind a, she pulls a telescoping, 8.2s"
  duration_seconds: 8.2
  aspect_ratio: "16:9"

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

STEP 1: LOAD SETTING IMAGE (CACHED)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Lookup: visual_bible_manifest.json                             â”‚
  â”‚   â†’ settings["east_la_strip_mall_parking_lot"]                 â”‚
  â”‚   â†’ image_path: "settings/east_la_strip_mall_parking_lot.png"  â”‚
  â”‚                                                                 â”‚
  â”‚ IF exists:                                                      â”‚
  â”‚   setting_img = load_image(image_path)  âœ… HIT CACHE           â”‚
  â”‚ ELSE:                                                           â”‚
  â”‚   # Generate ONCE per location (first time only)               â”‚
  â”‚   t2i_prompt = visual_bible.location_designs[...].t2i_base_prompt
  â”‚   # "Photorealistic East Los Angeles strip mall parking lot   â”‚
  â”‚   #  at night, cracked asphalt with oil stains, low stucco    â”‚
  â”‚   #  storefronts (payday lender, phone repair), buzzing neon, â”‚
  â”‚   #  sodium vapor streetlights, deep shadows, heat haze..."   â”‚
  â”‚                                                                 â”‚
  â”‚   setting_img = flux_t2i(                                       â”‚
  â”‚     prompt=t2i_prompt,                                          â”‚
  â”‚     width=1920, height=1080,                                    â”‚
  â”‚     steps=50, guidance=7.5                                      â”‚
  â”‚   )                                                             â”‚
  â”‚   save_to_cache(setting_img, image_path)                       â”‚
  â”‚   update_manifest()                                             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  OUTPUT: setting_img (1920x1080 base environment, NO people)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

STEP 2: LOAD CHARACTER PORTRAIT (CACHED)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ For each character_ref_id in shot.character_ref_ids:           â”‚
  â”‚   Lookup: visual_bible_manifest.json                           â”‚
  â”‚     â†’ characters["rae_calder"]                                  â”‚
  â”‚     â†’ image_path: "characters/rae_calder.png"                   â”‚
  â”‚                                                                 â”‚
  â”‚   IF exists:                                                    â”‚
  â”‚     char_img = load_image(image_path)  âœ… HIT CACHE            â”‚
  â”‚   ELSE:                                                         â”‚
  â”‚     # Generate ONCE per character (first appearance)           â”‚
  â”‚     t2i_prompt = visual_bible.character_visual_notes[...].t2i_portrait_prompt
  â”‚     # "Photorealistic character portrait of Rae Calder, lean  â”‚
  â”‚     #  wiry-strong woman with light olive skin, dark brown    â”‚
  â”‚     #  shoulder-length hair in messy knot, faint crescent     â”‚
  â”‚     #  scar under right jaw, neutral expression, standing     â”‚
  â”‚     #  against blank background, 35mm portrait lens..."       â”‚
  â”‚                                                                 â”‚
  â”‚     char_img = flux_t2i(                                        â”‚
  â”‚       prompt=t2i_prompt,                                        â”‚
  â”‚       width=1024, height=1536,  # portrait ratio               â”‚
  â”‚       steps=50, guidance=7.5                                    â”‚
  â”‚     )                                                           â”‚
  â”‚     save_to_cache(char_img, image_path)                        â”‚
  â”‚     update_manifest()                                           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  OUTPUT: char_imgs[] (list of character portraits, background removed)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

STEP 3: INPAINT CHARACTERS INTO SETTING (I2I)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Composite characters into setting using scene_prompt:          â”‚
  â”‚                                                                 â”‚
  â”‚ composed_img = sd_inpaint(                                      â”‚
  â”‚   base_image=setting_img,  # the parking lot                   â”‚
  â”‚   character_images=[rae_calder.png],                            â”‚
  â”‚   edit_prompt=shot.scene_prompt,                                â”‚
  â”‚   # "RAE CALDER (lean, wiry-strong...) center frame,           â”‚
  â”‚   #  figure exits the taqueria, medium shot, subject at        â”‚
  â”‚   #  medium distance, predatory, working-class, Sodium amber"  â”‚
  â”‚   strength=0.6,  # blend factor                                 â”‚
  â”‚   guidance=7.0,                                                 â”‚
  â”‚   negative_prompt=shot.negative_prompt                          â”‚
  â”‚   # "blurry, low quality, cartoon, anime, neon-cyberpunk..."   â”‚
  â”‚ )                                                               â”‚
  â”‚                                                                 â”‚
  â”‚ # Alternative: ControlNet for precise placement                â”‚
  â”‚ composed_img = controlnet_inpaint(                              â”‚
  â”‚   base=setting_img,                                             â”‚
  â”‚   char_masks=generate_masks_from_prompt(scene_prompt),         â”‚
  â”‚   char_imgs=[rae_calder.png],                                   â”‚
  â”‚   prompt=scene_prompt,                                          â”‚
  â”‚   controlnet_model="sd15_inpaint"                               â”‚
  â”‚ )                                                               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  OUTPUT: composed_img (setting WITH characters in frame, STILL IMAGE)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

STEP 4: GENERATE VIDEO CLIP (I2V with Google Veo / Runway)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Send composed_img + motion prompt to I2V API:                  â”‚
  â”‚                                                                 â”‚
  â”‚ video_clip = google_veo_2(                                      â”‚
  â”‚   init_image=composed_img,  # from Step 3                      â”‚
  â”‚   motion_prompt=shot.video_prompt,                              â”‚
  â”‚   # "camera slowly pushing in, figure exits the taqueria,      â”‚
  â”‚   #  rae slides behind a, she pulls a telescoping, 8.2s"       â”‚
  â”‚   #  âš ï¸ BUG: this is garbage text, motion extraction broken    â”‚
  â”‚   duration_seconds=8.2,                                         â”‚
  â”‚   aspect_ratio="16:9",                                          â”‚
  â”‚   fps=24,                                                       â”‚
  â”‚   motion_strength=0.7,  # how much motion vs static            â”‚
  â”‚   quality="production"                                          â”‚
  â”‚ )                                                               â”‚
  â”‚                                                                 â”‚
  â”‚ # Alternative APIs:                                             â”‚
  â”‚ # - runway_gen3(init_image, text_prompt, duration)             â”‚
  â”‚ # - pika_labs_i2v(init_image, motion_prompt, duration)         â”‚
  â”‚ # - kling_ai_i2v(init_image, motion_prompt, duration)          â”‚
  â”‚ # - luma_dream_machine(init_image, prompt, duration)           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  OUTPUT: video_clip (8.2s silent MP4, 1920x1080@24fps)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

STEP 5: GENERATE & MIX AUDIO (PARALLEL)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ A) Dialogue (IF shot.dialogue_text exists):                    â”‚
  â”‚    speaker = shot.dialogue_speaker  # "Rae"                    â”‚
  â”‚    voice_id = character_voice_map[speaker]  # from manifest   â”‚
  â”‚    tts_audio = elevenlabs.generate(                            â”‚
  â”‚      text=shot.dialogue_text,                                  â”‚
  â”‚      voice=voice_id,                                            â”‚
  â”‚      model="eleven_turbo_v2"                                    â”‚
  â”‚    )                                                            â”‚
  â”‚                                                                 â”‚
  â”‚ B) Ambient audio:                                              â”‚
  â”‚    ambient_prompt = shot.ambient_description                   â”‚
  â”‚    # "outdoor ambiance, crickets, distant traffic, warm       â”‚
  â”‚    #  plastic smell from overheating phone chargers"          â”‚
  â”‚    ambient_audio = audiocraft_gen(                             â”‚
  â”‚      prompt=ambient_prompt,                                    â”‚
  â”‚      duration=8.2,                                              â”‚
  â”‚      model="audiogen-medium"                                    â”‚
  â”‚    )                                                            â”‚
  â”‚                                                                 â”‚
  â”‚ C) Mix layers:                                                 â”‚
  â”‚    final_audio = mix_audio(                                    â”‚
  â”‚      dialogue=tts_audio if exists else None,                  â”‚
  â”‚      ambient=ambient_audio,                                    â”‚
  â”‚      music=None,  # TODO: score engine                         â”‚
  â”‚      levels={"dialogue": 0.8, "ambient": 0.3}                  â”‚
  â”‚    )                                                            â”‚
  â”‚                                                                 â”‚
  â”‚ D) Attach to video:                                            â”‚
  â”‚    video_clip.set_audio(final_audio)                           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  OUTPUT: video_clip WITH AUDIO (8.2s MP4 with dialogue + ambient)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

STEP 6: SAVE CLIP TO DISK
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ output_path = f"artifacts/{project_id}/clips/{shot.shot_id}.mp4"
  â”‚ # "artifacts/sp_rae_blackout_20260214_100612/clips/s001_003.mp4"
  â”‚                                                                 â”‚
  â”‚ video_clip.save(                                                â”‚
  â”‚   output_path,                                                  â”‚
  â”‚   codec="h264",                                                 â”‚
  â”‚   bitrate="8M",                                                 â”‚
  â”‚   audio_codec="aac",                                            â”‚
  â”‚   audio_bitrate="192k"                                          â”‚
  â”‚ )                                                               â”‚
  â”‚                                                                 â”‚
  â”‚ # Update shot manifest:                                        â”‚
  â”‚ shot_manifest.json:                                             â”‚
  â”‚   {                                                             â”‚
  â”‚     "s001_003": {                                               â”‚
  â”‚       "status": "completed",                                    â”‚
  â”‚       "clip_path": "clips/s001_003.mp4",                        â”‚
  â”‚       "duration": 8.2,                                          â”‚
  â”‚       "generated_at": "2026-02-16T10:34:22Z"                    â”‚
  â”‚     }                                                            â”‚
  â”‚   }                                                             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  OUTPUT: clips/s001_003.mp4 SAVED âœ…

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

REPEAT for all 754 shots â†’ 754 MP4 clips in clips/ directory

THEN: Scene Stitcher concatenates clips with transitions
THEN: Final Assembly produces output/{project_id}_FINAL.mp4
```

---

## 3. COMPREHENSIVE GAP ANALYSIS

### 3.1 WHAT EXISTS TODAY âœ…

| Component | Status | Location | Notes |
|-----------|--------|----------|-------|
| **Screenplay Engine** | âœ… COMPLETE | `src/screenplay_engine/` | 9 steps, 970 tests passing |
| **World Bible** | âœ… COMPLETE | `src/screenplay_engine/pipeline/steps/step_3b_world_bible.py` | Arena, geography, culture, economy |
| **Full Cast (Layered)** | âœ… COMPLETE | `src/screenplay_engine/pipeline/steps/step_3c_full_cast.py` | 3 tiers, layered API calls |
| **Visual Bible** | âœ… COMPLETE | `src/screenplay_engine/pipeline/steps/step_5b_visual_bible.py` | Style bible, color script, location designs, character portraits |
| **Shot Engine** | âœ… COMPLETE | `src/shot_engine/` | 6-step pipeline (V1-V6) |
| - V1: Scene Decomposition | âœ… WORKING | `src/shot_engine/pipeline/steps/step_v1_decomposition.py` | Parses scenes into shots |
| - V2: Shot Type Assignment | âœ… WORKING | `src/shot_engine/pipeline/steps/step_v2_shot_types.py` | Maps trigger â†’ shot type |
| - V3: Camera Behavior | âœ… WORKING | `src/shot_engine/pipeline/steps/step_v3_camera.py` | Camera movement, lens, height |
| - V4: Duration & Pacing | âœ… WORKING | `src/shot_engine/pipeline/steps/step_v4_pacing.py` | Timing per shot |
| - V5: Transitions | âœ… WORKING | `src/shot_engine/pipeline/steps/step_v5_transitions.py` | Cut/dissolve/fade rules |
| - V6: Prompt Generation | âš ï¸ BUGGY | `src/shot_engine/pipeline/steps/step_v6_prompts.py` | Generates prompts but has bugs |
| **Shot List Output** | âœ… COMPLETE | `artifacts/{project_id}/shot_list.json` | 754 shots with metadata |

### 3.2 WHAT IS BROKEN/NEEDS FIXING ğŸ”§

#### 3.2.1 V6 Prompt Generation Bugs (CRITICAL)

**BUG 1: Garbage video_prompt (motion extraction failure)**

**Evidence:**
```json
{
  "shot_id": "s001_005",
  "video_prompt": "camera tracking movement, figure skip turns, they slam into the, 3.6s"
}
```

**Root Cause:**
In `step_v6_prompts.py`, the `_extract_motion()` function uses regex to extract motion verbs but fails when:
- Sentence fragments are passed instead of full sentences
- Context is incomplete (e.g., "skip turns" is a character name + verb, not a motion phrase)
- The motion phrase includes incomplete objects ("slam into the" vs "slam into the dumpster")

**Impact:** 2/754 shots (0.3%) have malformed video prompts
**Severity:** HIGH â€” Veo API will produce nonsense motion from these prompts

**Fix Required:**
1. Improve sentence boundary detection in `_extract_motion()`
2. Add context window expansion (grab full sentence, not just verb window)
3. Add validation: reject prompts with trailing prepositions ("into the", "from the")
4. Fallback: if extraction fails, use trigger-based default motion

---

**BUG 2: characters_in_frame frequently empty when characters ARE in shot**

**Evidence:**
- 296/754 shots (39%) have empty `characters_in_frame` despite character names in `content`
- Example: `content: "Rae shoves him behind the dumpster"` â†’ `characters_in_frame: []`

**Root Cause:**
In `step_v1_decomposition.py`, `_extract_characters()` uses word-boundary regex but fails when:
- Character names appear in possessive form ("Rae's" not matching "Rae")
- Character names appear in lowercase action text
- Known character list is incomplete (missing tier 2/3 characters)

**Impact:** 296 shots missing character metadata
**Severity:** CRITICAL â€” Shots won't trigger character inpainting in video engine

**Fix Required:**
1. Build complete character name list from Step 3 + 3c artifacts
2. Add fuzzy matching (possessive, lowercase, partial name)
3. Add character detection from dialogue_speaker field (if present, add to frame)
4. Validate: if `dialogue_speaker` exists, they MUST be in `characters_in_frame`

---

**BUG 3: "speaking, speaking" duplication in dialogue shots**

**Evidence:** Not found in sample but mentioned in user request

**Root Cause (hypothetical):**
Likely in `_build_blocking_description()` or scene_prompt generation where dialogue trigger adds "speaking" and then video_prompt adds it again.

**Fix Required:**
1. Grep for duplication pattern in `step_v6_prompts.py`
2. Deduplicate keywords in prompt assembly
3. Add unit test to catch duplicate terms

---

### 3.3 WHAT IS MISSING (TO BE BUILT) ğŸš§

#### 3.3.1 Visual Bible Asset Manager

**Status:** âŒ NOT IMPLEMENTED
**Priority:** P0 (BLOCKING)
**Estimated Effort:** 3-5 days

**Required Components:**

1. **Asset Index Builder**
   - Input: `sp_step_5b_visual_bible.json`
   - Output: Normalized lookup tables
   - Functions:
     - `build_settings_index()` â†’ `{setting_ref_id: {t2i_prompt, mood, colors}}`
     - `build_characters_index()` â†’ `{char_ref_id: {t2i_portrait_prompt, physical_appearance}}`

2. **Setting Image Generator (T2I)**
   - API: Flux Pro / Stable Diffusion XL
   - Cache: `artifacts/{project_id}/settings/{ref_id}.png`
   - Run ONCE per unique location (e.g., 12 locations for 40-scene screenplay)
   - Estimated cost: $0.05/image Ã— 12 = $0.60 per project

3. **Character Portrait Generator (T2I)**
   - API: Flux Pro / Stable Diffusion XL
   - Cache: `artifacts/{project_id}/characters/{ref_id}.png`
   - Run ONCE per unique character (e.g., 15 named characters)
   - Estimated cost: $0.05/image Ã— 15 = $0.75 per project
   - Special: Remove background for inpainting (rembg library)

4. **Manifest Writer**
   - Output: `visual_bible_manifest.json`
   - Schema:
     ```json
     {
       "settings": {
         "setting_ref_id": {
           "image_path": "settings/{ref_id}.png",
           "prompt": "...",
           "generated_at": "...",
           "width": 1920,
           "height": 1080
         }
       },
       "characters": {
         "char_ref_id": {
           "image_path": "characters/{ref_id}.png",
           "prompt": "...",
           "generated_at": "...",
           "background_removed": true
         }
       }
     }
     ```

**Files to Create:**
- `src/visual_bible_engine/asset_manager.py`
- `src/visual_bible_engine/t2i_client.py` (Flux/SD API wrapper)
- `src/visual_bible_engine/manifest.py` (read/write manifest)

---

#### 3.3.2 Video Engine (MOST CRITICAL)

**Status:** âŒ NOT IMPLEMENTED
**Priority:** P0 (BLOCKING EVERYTHING)
**Estimated Effort:** 2-3 weeks

**Required Components:**

1. **Shot Processor**
   - Input: `shot_list.json` + `visual_bible_manifest.json`
   - Output: 754 MP4 clips in `artifacts/{project_id}/clips/`
   - Per-shot pipeline:
     1. Load cached setting image
     2. Load cached character portrait(s)
     3. Inpaint characters into setting (I2I)
     4. Generate video from composed image (I2V)
     5. Generate dialogue audio (TTS)
     6. Generate ambient audio
     7. Mix audio layers
     8. Attach audio to video
     9. Save clip to disk

2. **Image Inpainting Module (I2I)**
   - API Options:
     - Stable Diffusion Inpaint (local)
     - Flux Inpaint (API, better quality)
     - ControlNet (precise character placement)
   - Input: setting image + character images + scene_prompt
   - Output: Composed still frame (characters in setting)
   - Estimated time: 3-5s per shot (local GPU)

3. **Video Generation Module (I2V)**
   - API Options (PICK ONE):
     - **Google Veo 2** (best quality, expensive, slow)
     - **Runway Gen-3** (good quality, moderate cost)
     - **Pika Labs** (fast, lower quality)
     - **Kling AI** (good for Asian markets)
     - **Luma Dream Machine** (free tier available)
   - Input: composed image + video_prompt + duration
   - Output: Silent video clip (MP4, 1920x1080, 24fps)
   - Estimated time: 30-120s per shot (API-dependent)
   - Estimated cost: $0.10-$0.50 per shot â†’ $75-$377 per feature film

4. **Audio Pipeline**
   - **Dialogue TTS:**
     - API: ElevenLabs / Azure TTS / Google TTS
     - Character voice mapping (from visual bible or manual config)
     - Lip sync (optional): Wav2Lip model
   - **Ambient Audio:**
     - API: AudioCraft (Meta) / Stable Audio
     - Input: `shot.ambient_description`
   - **Audio Mixer:**
     - Library: pydub / moviepy
     - Layer dialogue + ambient + music (future)
     - Export: 192kbps AAC

5. **Shot Stitcher**
   - Library: moviepy / ffmpeg
   - Input: All clips for a scene
   - Apply transitions (cut/dissolve/fade)
   - Output: Scene-level MP4 files

6. **Final Assembly**
   - Concatenate all scenes
   - Add act transitions (fade to black)
   - Add opening/closing titles (from Step 9 marketing)
   - Export final feature film MP4

**Files to Create:**
- `src/video_engine/shot_processor.py`
- `src/video_engine/i2i_inpaint.py` (SD/Flux inpainting wrapper)
- `src/video_engine/i2v_client.py` (Veo/Runway API wrapper)
- `src/video_engine/audio_pipeline.py` (TTS + ambient + mixing)
- `src/video_engine/stitcher.py` (clip concatenation + transitions)
- `src/video_engine/orchestrator.py` (main execution loop)

**API Keys Required:**
- Flux API key (T2I + Inpaint)
- Google Veo API key (I2V) OR Runway OR Pika
- ElevenLabs API key (TTS)
- AudioCraft (local) OR Stable Audio API

---

#### 3.3.3 Audio Engine (Separate Pipeline)

**Status:** âŒ NOT IMPLEMENTED
**Priority:** P1 (can run separately after video)
**Estimated Effort:** 1 week

**Components:**
1. **Character Voice Mapping**
   - Map `dialogue_speaker` â†’ ElevenLabs voice_id
   - Store in config: `voice_map.json`

2. **TTS Generator**
   - API: ElevenLabs (best quality)
   - Fallback: Azure TTS, Google TTS

3. **Ambient Audio Generator**
   - Use AudioCraft (local, free)
   - OR Stable Audio API

4. **Music Score Engine (FUTURE)**
   - AI music generation for scenes
   - Beat-aligned transitions

5. **Lip Sync (OPTIONAL)**
   - Wav2Lip model for dialogue shots
   - Requires face detection in video

---

#### 3.3.4 Asset Cache System

**Status:** âŒ NOT IMPLEMENTED
**Priority:** P0 (CRITICAL FOR COST SAVINGS)
**Estimated Effort:** 2 days

**Why Critical:**
Without caching, regenerating setting images and character portraits for every shot would cost:
- 754 shots Ã— $0.05/image = $37.70 (vs $1.35 with caching)
- Processing time: 754 shots Ã— 5s = 63 minutes (vs 1 minute with caching)

**Components:**
1. **Cache Storage**
   - Directory: `artifacts/{project_id}/cache/`
   - Subdirs: `settings/`, `characters/`, `clips/`

2. **Cache Lookup**
   - Before generating any image, check cache
   - Use normalized ref_id as key

3. **Cache Invalidation**
   - If visual bible changes, mark settings/characters as stale
   - If screenplay changes, mark clips as stale

4. **Manifest Tracking**
   - `visual_bible_manifest.json` tracks all cached assets
   - `shot_manifest.json` tracks all generated clips

**Files to Create:**
- `src/cache/cache_manager.py`
- `src/cache/manifest_io.py`

---

#### 3.3.5 Clip Stitcher & Final Assembly

**Status:** âŒ NOT IMPLEMENTED
**Priority:** P1 (needed for full output)
**Estimated Effort:** 3 days

**Components:**

1. **Scene Stitcher**
   - Input: All clips for a scene (e.g., 18 clips for Scene 1)
   - Apply transitions:
     - `cut`: instant (no blend)
     - `dissolve`: crossfade over `crossfade_duration`
     - `fade_black`: fade out â†’ fade in
   - Output: `scenes/scene_001.mp4`

2. **Act Transitions**
   - Between acts: 2-second fade to black

3. **Opening/Closing Titles**
   - Pull from `sp_step_9_marketing.json`:
     - `title_sequence` (opening)
     - `poster_tagline` (closing card)
   - Generate title cards with Pillow/moviepy
   - Insert at start/end of film

4. **Final Export**
   - Concatenate: titles + all scenes + credits
   - Export: `output/{project_id}_FINAL.mp4`
   - Codec: H.264, 8Mbps, 1920x1080, 24fps
   - Audio: AAC, 192kbps, stereo

**Files to Create:**
- `src/video_engine/scene_stitcher.py`
- `src/video_engine/title_generator.py`
- `src/video_engine/final_assembly.py`

---

### 3.4 ESTIMATED COSTS (Per Feature Film)

| Item | Unit Cost | Quantity | Total |
|------|-----------|----------|-------|
| **T2I (Settings)** | $0.05/image | 12 unique locations | $0.60 |
| **T2I (Characters)** | $0.05/image | 15 characters | $0.75 |
| **I2V (Veo 2)** | $0.30/shot | 754 shots | $226.20 |
| **TTS (ElevenLabs)** | $0.001/char | ~50K characters | $50.00 |
| **Ambient Audio** | Local (free) | - | $0.00 |
| **TOTAL (Veo)** | | | **$277.55** |
| | | | |
| **I2V (Runway Gen-3)** | $0.10/shot | 754 shots | $75.40 |
| **TOTAL (Runway)** | | | **$126.75** |
| | | | |
| **I2V (Pika Labs)** | $0.05/shot | 754 shots | $37.70 |
| **TOTAL (Pika)** | | | **$89.05** |

**Recommended:** Start with Pika Labs for prototyping ($89/film), upgrade to Runway ($127/film) for production quality.

---

### 3.5 ESTIMATED PROCESSING TIME (Per Feature Film)

| Phase | Time per Shot | Total Shots | Total Time |
|-------|--------------|-------------|------------|
| **Asset Generation** (settings + chars) | 5s | 27 images | **2.25 min** |
| **I2I Inpainting** (local GPU) | 4s | 754 shots | **50 min** |
| **I2V Generation** (Veo API) | 60s | 754 shots | **12.6 hours** |
| **I2V Generation** (Runway API) | 30s | 754 shots | **6.3 hours** |
| **I2V Generation** (Pika API) | 15s | 754 shots | **3.2 hours** |
| **Audio Generation** (TTS + ambient) | 3s | 754 shots | **38 min** |
| **Audio Mixing** | 1s | 754 shots | **13 min** |
| **Clip Stitching** | 0.5s | 40 scenes | **20 sec** |
| **Final Assembly** | - | 1 film | **2 min** |
| | | | |
| **TOTAL (Pika, parallel)** | | | **~5 hours** |
| **TOTAL (Runway, parallel)** | | | **~8 hours** |
| **TOTAL (Veo, parallel)** | | | **~14 hours** |

**Note:** With API parallelization (10 concurrent requests), total time reduces by 50-70%.

---

### 3.6 PRIORITY ORDER (What to Build First)

1. **P0 (BLOCKING):**
   - Fix V6 bugs (motion extraction, character detection) â€” **1 day**
   - Build Visual Bible Asset Manager â€” **3 days**
   - Build Video Engine core (shot processor + I2I + I2V) â€” **1 week**

2. **P1 (HIGH):**
   - Build Audio Pipeline (TTS + ambient + mixing) â€” **3 days**
   - Build Cache System â€” **2 days**
   - Build Scene Stitcher â€” **2 days**

3. **P2 (MEDIUM):**
   - Build Final Assembly (titles + export) â€” **2 days**
   - Add Lip Sync (Wav2Lip) â€” **3 days**

4. **P3 (LOW):**
   - Build Music Score Engine â€” **1 week**
   - Add Veo API fallback logic â€” **1 day**

---

### 3.7 RECOMMENDED TECH STACK

```python
# Core Dependencies
{
  "video_generation": "google-veo-api OR runwayml OR pika-labs",
  "image_generation": "flux-pro OR stable-diffusion-xl",
  "inpainting": "controlnet OR flux-inpaint",
  "tts": "elevenlabs OR azure-tts",
  "audio_generation": "audiocraft (local) OR stable-audio",
  "video_editing": "moviepy OR ffmpeg-python",
  "audio_mixing": "pydub",
  "background_removal": "rembg",
  "cache": "filesystem + JSON manifest",
  "parallelization": "concurrent.futures OR asyncio"
}
```

---

## 4. CRITICAL ISSUES SUMMARY

### 4.1 Shot Engine Bugs (MUST FIX)

1. **Video prompt garbage (2/754 shots):**
   - "figure skip turns, they slam into the" â†’ nonsense
   - Fix: Improve `_extract_motion()` sentence parsing

2. **Missing character detection (296/754 shots):**
   - Empty `characters_in_frame` despite characters in `content`
   - Fix: Improve `_extract_characters()` fuzzy matching + add from `dialogue_speaker`

3. **Speaking duplication (unknown count):**
   - "speaking, speaking" in prompts
   - Fix: Deduplicate keywords in prompt assembly

### 4.2 Missing Components (MUST BUILD)

1. **Visual Bible Asset Manager:**
   - No system to generate/cache setting images
   - No system to generate/cache character portraits

2. **Video Engine:**
   - No I2I inpainting module
   - No I2V API client
   - No shot processor loop

3. **Audio Pipeline:**
   - No TTS integration
   - No ambient audio generation
   - No audio mixing

4. **Asset Cache:**
   - No caching layer (will waste money regenerating images)

5. **Stitcher:**
   - No scene assembly
   - No final film export

---

## 5. NEXT STEPS (Actionable Plan)

### Week 1: Fix Shot Engine + Build Asset Manager
- [ ] Day 1: Fix V6 motion extraction bug
- [ ] Day 2: Fix V6 character detection bug
- [ ] Day 3-5: Build Visual Bible Asset Manager (T2I settings + characters)

### Week 2: Build Video Engine Core
- [ ] Day 1-2: Build I2I inpainting module (SD/Flux)
- [ ] Day 3-4: Build I2V API client (Pika Labs for prototyping)
- [ ] Day 5: Build shot processor loop (integrate I2I + I2V)

### Week 3: Audio + Stitching
- [ ] Day 1-2: Build TTS pipeline (ElevenLabs)
- [ ] Day 3: Build ambient audio generation (AudioCraft)
- [ ] Day 4: Build audio mixer
- [ ] Day 5: Build scene stitcher + final assembly

### Week 4: Testing + Optimization
- [ ] Day 1-2: End-to-end test (generate 5-minute short film)
- [ ] Day 3: Optimize caching + parallelization
- [ ] Day 4: Cost/quality analysis (Pika vs Runway vs Veo)
- [ ] Day 5: Documentation + deployment guide

---

## 6. ARCHITECTURE NOTES

### 6.1 Setting-Per-Location Architecture

**Key Insight from v12.0.0:**
The visual bible now generates ONE setting design per UNIQUE LOCATION (not per scene). This is critical for:

1. **Cost savings:** 12 setting images instead of 40 (one per scene)
2. **Continuity:** Same parking lot image used across 5 scenes
3. **Cache efficiency:** Setting images reused across multiple shots

**How it works:**
- `location_designs[]` in visual bible has `time_variants` dict
- Shot engine normalizes slugline â†’ `setting_ref_id`
- Visual Bible Asset Manager generates base image ONCE per location
- Video Engine applies time-of-day variants via I2I editing if needed

### 6.2 Character State Tracking

**Challenge:** Characters change wardrobe, injuries, etc. across acts.

**Solution (Future):**
- Visual bible `character_visual_notes[]` includes `wardrobe_evolution`
- Asset Manager generates MULTIPLE portraits per character:
  - `rae_calder_act1.png` (initial wardrobe)
  - `rae_calder_act2.png` (bloodied, torn jacket)
  - `rae_calder_act3.png` (cleaned up, different clothes)
- Shot processor selects variant based on `shot.beat` â†’ act mapping

### 6.3 Parallelization Strategy

**Single-threaded time:** 14 hours for Veo
**10-parallel time:** 1.4 hours (10x speedup)

**Implementation:**
```python
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=10) as executor:
    futures = [
        executor.submit(process_shot, shot)
        for shot in shot_list.all_shots()
    ]
    results = [f.result() for f in futures]
```

**Note:** API rate limits may cap parallelization (e.g., Veo: 5 concurrent)

---

## END OF DOCUMENT

**Generated:** 2026-02-16
**Version:** 1.0.0
**Next Review:** After V6 bugs fixed + Asset Manager built
